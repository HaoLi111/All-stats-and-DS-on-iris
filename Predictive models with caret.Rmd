---
title: "Predictive models with caret"
author: "Hao Li"
date: "20190620"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Why? A huge blackbox without understanding?

It is generally not encouraged, huh?

True, it is unsafe and unreasonable to use an unknown model to predict complexed thing,... but

You can establish some understanding of complexity of the data, by first
apply and auto-tune some powerful and complexed model for the predictions,
and then use a relatively simple model which is explained.

Compare the predictive model and the explainatory one...

What does the comparision suggest?

 - The limitation of the predictive model suggest how much randomness is still not well presented in the data. e.g. Is there more hidden layers (partials) yet to be considered?
 
 - When you choose a simpler model that is easy to explain. The predicting power you lose is a measure of how much complexity is not considered in the explainatory model.

And that could be important. Wait, I haven't shown how to automatically deploy and tune these big blackboxes yet!


## CARET package

```
#install.packages("caret")
#install.packages("visreg")
```

## Default model selection

```{r}
library(caret)
library(visreg)
```


```{r}
model1 =train(Species ~ ., data = iris)
model2 =train(Sepal.Length~.,data=iris)
```

```{r}

pred1 = predict(model1)
plot(model1)

#visreg::visreg(pred1)
caret::confusionMatrix(iris$Species,pred1)
#install.packages('broom')
str(model1)

```



```{r}
pred2 = predict(model2)
visreg::visreg(model2)
```

## Lookup tunable parameters for the black boxes

```{r}
modelLookup('C5.0')
#modelLookup('tree')
modelLookup('rf')
modelLookup('nnet')
#getModelInfo('nnet')
#modelLookup()
```


## tune a C5.0 tree

```{r trainControl}
ctrl = trainControl(method ='cv',number = 20,selectionFunction = 'oneSE')

#getModelInfo('C5.0')
ctrlgrid = as.data.frame(expand.grid(trials =seq(from=1,to=26,by=5),model ='tree',winnow=FALSE))
colnames(ctrlgrid)

ctrl_model =train(Species~.,data=iris,method = "C5.0",.metric = "Kappa",
                  trControl =ctrl,
                  tuneGrid = ctrlgrid)

ctrl_model

```


## tune a NN

```{r}
modelLookup('nnet')
modelLookup('nnet')[1,3]
```

```{r}
Ctrl2 = trainControl()
nnet_iris_tune = train(Species~.,data=iris,method = 'nnet',
                       trControl = trainControl(method = 'cv'),
                       tuneGrid = expand.grid(size = 1:4,
                                              decay = 0),
                       metric = 'Kappa')
nnet_iris_tune

#str(nnet_iris_tune)
plot(nnet_iris_tune)
Perf= as.matrix(nnet_iris_tune$results)
```


# Bagging

```{r}

library(caret)
library(ipred)
ctrl = trainControl(method ='cv',number = 10)
train(Species~.,data =iris,method = 'treebag',
      trControl = ctrl)

```


```{r baggedSVM}
str(svmBag)
ctrl = bagControl(fit = svmBag$fit,predict = svmbag$pred,aggregate = svmBag$aggregate)


iris_1=within(iris,{
  is_setosa = ifelse(iris$Species=='setosa',1,0)})
iris_1$Species=NULL
svmbag = train(is_setosa~.,data = iris_1,method = "bag",
               bagControl = ctrl)


```


# Boosting

##adaboost

```{r}
library(adabag)
library(doParallel)
registerDoParallel(6)
getDoParWorkers()


#system.time({
 # adaboost_iris = train(Species~.,data = iris,method = 'AdaBoost.M1')
#})

plot(adaboost_iris)

```